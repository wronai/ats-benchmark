```json
{
  "diagnosis": "The codebase lacks a LogFlowParser class needed for distributed tracing analysis. The missing component should parse structured logs, group entries by trace_id to reconstruct request flows, and compress these flows into a compact graph representation suitable for LLM consumption. This is essential for debugging and analyzing data flow across microservices.",
  "fixed_files": {
    "app/log_flow_parser.py": "\"\"\"LogFlowParser - Parser for distributed tracing logs with LLM-optimized graph compression.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Set, Tuple\n\n\n@dataclass\nclass LogEntry:\n    \"\"\"Represents a single log entry in a distributed system.\"\"\"\n    timestamp: str\n    service: str\n    trace_id: str\n    span_id: str\n    parent_span_id: Optional[str] = None\n    message: str = \"\"\n    level: str = \"INFO\"\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass FlowNode:\n    \"\"\"Node in the flow graph representing a service call.\"\"\"\n    span_id: str\n    service: str\n    timestamp: str\n    message: str\n    level: str\n    duration_ms: Optional[float] = None\n    children: List[FlowNode] = field(default_factory=list)\n\n\nclass LogFlowParser:\n    \"\"\"Parser for log data flow analysis with trace grouping and LLM compression.\"\"\"\n    \n    def __init__(self):\n        self.traces: Dict[str, List[LogEntry]] = defaultdict(list)\n        self._span_index: Dict[str, LogEntry] = {}\n    \n    def parse_log_line(self, line: str) -> Optional[LogEntry]:\n        \"\"\"Parse a single JSON log line into LogEntry.\"\"\"\n        try:\n            data = json.loads(line)\n            return LogEntry(\n                timestamp=data.get(\"timestamp\", \"\"),\n                service=data.get(\"service\", \"unknown\"),\n                trace_id=data.get(\"trace_id\", \"\"),\n                span_id=data.get(\"span_id\", \"\"),\n                parent_span_id=data.get(\"parent_span_id\"),\n                message=data.get(\"message\", \"\"),\n                level=data.get(\"level\", \"INFO\"),\n                metadata=data.get(\"metadata\", {})\n            )\n        except (json.JSONDecodeError, TypeError, KeyError):\n            return None\n    \n    def add_entry(self, entry: LogEntry) -> None:\n        \"\"\"Add a log entry to the parser, indexing by trace_id.\"\"\"\n        if not entry.trace_id or not entry.span_id:\n            return\n        self.traces[entry.trace_id].append(entry)\n        self._span_index[entry.span_id] = entry\n    \n    def parse_logs(self, logs: List[str]) -> int:\n        \"\"\"Parse multiple log lines, returns count of successfully parsed entries.\"\"\"\n        count = 0\n        for line in logs:\n            entry = self.parse_log_line(line)\n            if entry:\n                self.add_entry(entry)\n                count += 1\n        return count\n    \n    def group_by_trace_id(self) -> Dict[str, List[LogEntry]]:\n        \"\"\"Return logs grouped by trace_id.\"\"\"\n        return dict(self.traces)\n    \n    def build_flow_graph(self, trace_id: str) -> Dict[str, Any]:\n        \"\"\"Build a hierarchical flow graph for a specific trace.\"\"\"\n        entries = self.traces.get(trace_id, [])\n        if not entries:\n            return {}\n        \n        # Sort chronologically\n        entries.sort(key=lambda x: x.timestamp)\n        \n        # Build lookup maps\n        span_to_entry = {e.span_id: e for e in entries}\n        children_map: Dict[str, List[str]] = defaultdict(list)\n        root_spans: List[str] = []\n        \n        for entry in entries:\n            if entry.parent_span_id and entry.parent_span_id in span_to_entry:\n                children_map[entry.parent_span_id].append(entry.span_id)\n            else:\n                root_spans.append(entry.span_id)\n        \n        def build_node(span_id: str) -> FlowNode:\n            entry = span_to_entry[span_id]\n            return FlowNode(\n                span_id=span_id,\n                service=entry.service,\n                timestamp=entry.timestamp,\n                message=entry.message,\n                level=entry.level,\n                children=[build_node(child_id) for child_id in children_map.get(span_id, [])]\n            )\n        \n        root_nodes = [build_node(sid) for sid in root_spans]\n        services_involved = list(set(e.service for e in entries))\n        \n        return {\n            \"trace_id\": trace_id,\n            \"root_nodes\": root_nodes,\n            \"node_count\": len(entries),\n            \"services_involved\": services_involved,\n            \"start_time\": entries[0].timestamp if entries else None,\n            \"end_time\": entries[-1].timestamp if entries else None\n        }\n    \n    def compress_to_llm_format(self, trace_id: str, max_message_len: int = 80) -> str:\n        \"\"\"Compress trace into a compact graph format optimized for LLM context windows.\"\"\"\n        graph = self.build_flow_graph(trace_id)\n        if not graph:\n            return f\"Trace {trace_id}: No data\"\n        \n        lines = [\n            f\"Trace:{trace_id}\",\n            f\"Services:{','.join(graph['services_involved'])}\",\n            f\"Nodes:{graph['node_count']}\",\n            \"Flow:\"\n        ]\n        \n        def format_node(node: FlowNode, depth: int = 0) -> List[str]:\n            indent = \"  \" * depth\n            msg = node.message[:max_message_len] + \"...\" if len(node.message) > max_message_len else node.message\n            result = [f\"{indent}{node.service}[{node.level}]:{msg}\"]\n            for child in node.children:\n                result.extend(format_node(child, depth + 1))\n            return result\n        \n        for root in graph.get(\"root_nodes\", []):\n            lines.extend(format_node(root))\n        \n        return \"\\n\".join(lines)\n    \n    def compress_to_llm_json(self, trace_id: str) -> Dict[str, Any]:\n        \"\"\"Compress trace into a structured JSON format for LLM processing.\"\"\"\n        graph = self.build_flow_graph(trace_id)\n        if not graph:\n            return {}\n        \n        def node_to_dict(node: FlowNode) -> Dict[str, Any]:\n            return {\n                \"service\": node.service,\n                \"level\": node.level,\n                \"msg\": node.message[:100] + \"...\" if len(node.message) > 100 else node.message,\n                \"children\": [node_to_dict(c) for c in node.children]\n            }\n        \n        return {\n            \"trace_id\": trace_id,\n            \"services\": graph[\"services_involved\"],\n            \"flow\": [node_to_dict(n) for n in graph.get(\"root_nodes\", [])],\n            \"stats\": {\"nodes\": graph[\"node_count\"]}\n        }\n    \n    def get_trace_summary(self, trace_id: str) -> Dict[str, Any]:\n        \"\"\"Get summary statistics for a specific trace.\"\"\"\n        entries = self.traces.get(trace_id, [])\n        if not entries:\n            return {}\n        \n        services = set(e.service for e in entries)\n        levels = set(e.level for e in entries)\n        \n        # Calculate duration if timestamps are parseable\n        duration_sec = None\n        try:\n            timestamps = [\n                datetime.fromisoformat(e.timestamp.replace(\"Z\", \"+00:00\"))\n                for e in entries if e.timestamp\n            ]\n            if timestamps:\n                duration_sec = (max(timestamps) - min(timestamps)).total_seconds()\n        except (ValueError, TypeError):\n            pass\n        \n        return {\n            \"trace_id\": trace_id,\n            \"entry_count\": len(entries),\n            \"services_involved\": list(services),\n            \"levels\": list(levels),\n            \"duration_sec\": duration_sec,\n            \"error_count\": sum(1 for e in entries if e.level in (\"ERROR\", \"CRITICAL\", \"FATAL\"))\n        }\n    \n    def get_all_traces_summary(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get summary for all traces.\"\"\"\n        return {tid: self.get_trace_summary(tid) for tid in self.traces.keys()}\n    \n    def find_bottlenecks(self, trace_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Identify potential bottlenecks in the trace (services with most entries).\"\"\"\n        entries = self.traces.get(trace_id, [])\n        service_counts: Dict[str, int] = defaultdict(int)\n        for e in entries:\n            service_counts[e.service] += 1\n        \n        sorted_services = sorted(service_counts.items(), key=lambda x: x[1], reverse=True)\n        return [{\"service\": svc, \"calls\": count} for svc, count in sorted_services[:3]]\n",
    "tests/test_log_flow_parser.py": "import json\nimport pytest\nfrom app.log_flow_parser import LogFlowParser, LogEntry\n\n\nclass TestLogFlowParser:\n    \"\"\"Test suite for LogFlowParser functionality.\"\"\"\n    \n    @pytest.fixture\n    def sample_logs(self):\n        \"\"\"Generate sample distributed tracing logs.\"\"\"\n        return [\n            json.dumps({\n                \"timestamp\": \"2024-01-01T10:00:00Z\",\n                \"service\": \"api-gateway\",\n                \"trace_id\": \"trace-abc-123\",\n                \"span_id\": \"span-root\",\n                \"parent_span_id\": None,\n                \"message\": \"Incoming request GET /api/orders\",\n                \"level\": \"INFO\",\n                \"metadata\": {\"method\": \"GET\", \"path\": \"/api/orders\"}\n            }),\n            json.dumps({\n                \"timestamp\": \"2024-01-01T10:00:01Z\",\n                \"service\": \"auth-service\",\n                \"trace_id\": \"trace-abc-123\",\n                \"span_id\": \"span-auth\",\n                \"parent_span_id\": \"span-root\",\n                \"message\": \"Validating JWT token\",\n                \"level\": \"INFO\"\n            }),\n            json.dumps({\n                \"timestamp\": \"2024-01-01T10:00:02Z\",\n                \"service\": \"orders-service\",\n                \"trace_id\": \"trace-abc-123\",\n                \"span_id\": \"span-orders\",\n                \"parent_span_id\": \"span-auth\",\n                \"message\": \"Fetching orders from database\",\n                \"level\": \"DEBUG\"\n            }),\n            json.dumps({\n                \"timestamp\": \"2024-01-01T10:00:03Z\",\n                \"service\": \"database\",\n                \"trace_id\": \"trace-abc-123\",\n                \"span_id\": \"span-db\",\n                \"parent_span_id\": \"span-orders\",\n                \"message\": \"Executing SQL query: SELECT * FROM orders\",\n                \"level\": \"DEBUG\"\n            }),\n            json.dumps({\n                \"timestamp\": \"2024-01-01T10:00:04Z\",\n                \"service\": \"api-gateway\",\n                \"trace_id\": \"trace-xyz-789\",\n                \"span_id\": \"span-root-2\",\n                \"parent_span_id\": None,\n                \"message\": \"Incoming request POST /api/payments\",\n                \"level\": \"INFO\"\n            })\n        ]\n    \n    def test_parse_log_line_valid(self):\n        \"\"\"Test parsing of valid JSON log lines.\"\"\"\n        parser = LogFlowParser()\n        log_line = json.dumps({\n            \"timestamp\": \"2024-01-01T10:00:00Z\",\n            \"service\": \"test-service\",\n            \"trace_id\": \"trace-1\",\n            \"span_id\": \"span-1\",\n            \"message\": \"Test message\",\n            \"level\": \"INFO\"\n        })\n        \n        entry = parser.parse_log_line(log_line)\n        assert entry is not None\n        assert entry.service == \"test-service\"\n        assert entry.trace_id == \"trace-1\"\n        assert entry.span_id == \"span-1\"\n    \n    def test_parse_log_line_invalid(self):\n        \"\"\"Test handling of invalid log lines.\"\"\"\n        parser = LogFlowParser()\n        assert parser.parse_log_line(\"not json\") is None\n        assert parser.parse_log_line(\"\") is None\n        assert parser.parse_log_line(\"{invalid json\") is None\n    \n    def test_group_by_trace_id(self, sample_logs):\n        \"\"\"Test grouping logs by trace_id.\"\"\"\n        parser = LogFlowParser()\n        parser.parse_logs(sample_logs)\n        \n        grouped = parser.group_by_trace_id()\n        assert len(grouped) == 2\n        assert \"trace-abc-123\" in grouped\n        assert \"trace-xyz-789\" in grouped\n        assert len(grouped[\"trace-abc-123\"]) == 4\n        assert len(grouped[\"trace-xyz-789\"]) == 1\n    \n    def test_build_flow_graph(self, sample_logs):\n        \"\"\"Test building hierarchical flow graph.\"\"\"\n        parser = LogFlowParser()\n        parser.parse_logs(sample_logs)\n        \n        graph = parser.build_flow_graph(\"trace-abc-123\")\n        assert graph[\"trace_id\"] == \"trace-abc-123\"\n        assert graph[\"node_count\"] == 4\n        assert set(graph[\"services_involved\"]) == {\"api-gateway\", \"auth-service\", \"orders-service\", \"database\"}\n        assert len(graph[\"root_nodes\"]) == 1\n        \n        # Check hierarchy\n        root = graph[\"root_nodes\"][0]\n        assert root.service == \"api-gateway\"\n        assert len(root.children) == 1\n        assert root.children[0].service == \"auth-service\"\n    \n    def test_compress_to_llm_format(self, sample_logs):\n        \"\"\"Test LLM-optimized text compression.\"\"\"\n        parser = LogFlowParser()\n        parser.parse_logs(sample_logs)\n        \n        compressed = parser.compress_to_llm_format(\"trace-abc-123\")\n        assert \"Trace:trace-abc-123\" in compressed\n        assert \"api-gateway\" in compressed\n        assert \"auth-service\" in compressed\n        assert \"Flow:\" in compressed\n        # Check indentation for hierarchy\n        assert \"  auth-service\" in compressed\n    \n    def test_compress_to_llm_json(self, sample_logs):\n        \"\"\"Test LLM-optimized JSON compression.\"\"\"\n        parser = LogFlowParser()\n        parser.parse_logs(sample_logs)\n        \n        compressed = parser.compress_to_llm_json(\"trace-abc-123\")\n        assert compressed[\"trace_id\"] == \"trace-abc-123\"\n        assert \"api-gateway\" in compressed[\"services\"]\n        assert len(compressed[\"flow\"]) == 1\n        assert compressed[\"stats\"][\"nodes\"] == 4\n    \n    def test_get_trace_summary(self, sample_logs):\n        \"\"\"Test trace summary generation.\"\"\"\n        parser = LogFlowParser()\n        parser.parse_logs(sample_logs)\n        \n        summary = parser.get_trace_summary(\"trace-abc-123\")\n        assert summary[\"trace_id\"] == \"trace-abc-123\"\n        assert summary[\"entry_count\"] == 4\n        assert summary[\"duration_sec\"] == 3.0  # 10:00:00 to 10:00:03\n        assert summary[\"error_count\"] == 0\n    \n    def test_find_bottlenecks(self, sample_logs):\n        \"\"\"Test bottleneck detection.\"\"\"\n        parser = LogFlowParser()\n        parser.parse_logs(sample_logs)\n        \n        bottlenecks = parser.find_bottlenecks(\"trace-abc-123\")\n        assert len(bottlenecks) <= 3\n        assert bottlenecks[0][\"service\"] == \"api-gateway\"  # Most entries\n    \n    def test_empty_trace(self):\n        \"\"\"Test handling of non-existent trace.\"\"\"\n        parser = LogFlowParser()\n        assert parser.build_flow_graph(\"non-existent\") == {}\n        assert parser.compress_to_llm_format(\"non-existent\") == \"Trace non-existent: No data\"\n    \n    def test_parse_logs_count(self, sample_logs):\n        \"\"\"Test that parse_logs returns correct count.\"\"\"\n        parser = LogFlowParser()\n        count = parser.parse_logs(sample_logs)\n        assert count == 5\n        \n        # Add invalid log\n        count = parser.parse_logs(sample_logs + [\"invalid json\"])\n        assert count == 5  # Only valid ones counted\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n"
  },
  "test_code": "import json\nimport pytest\nfrom app.log_flow_parser import LogFlowParser, LogEntry\n\n\ndef test_log_flow_parser_end_to_end():\n    \"\"\"Integration test verifying trace grouping and LLM compression.\"\"\"\n    parser = LogFlowParser()\n    \n    # Simulate distributed logs\n    logs = [\n        json.dumps({\n            \"timestamp\": \"2024-01-15T10:00:00Z\",\n            \"service\": \"gateway\",\n            \"trace_id\": \"t-1\",\n            \"span_id\": \"s-1\",\n            \"parent_span_id\": None,\n           